# ğŸ“˜ **README.md â€” Quantization, LoRA, QLoRA & GGUF**

### *Efficient LLMs for Teaching, Research, and Deployment*

This repository demonstrates **quantization**, **LoRA**, **QLoRA**, and **GGUF-based inference** using Llama 3.2 and other small LLMs.
It includes:

* Python demos of **4-bit quantization**
* A full **QLoRA fine-tuning pipeline**
* **Inference using trained adapters**
* Running quantized **GGUF models** using `llama.cpp`

---

# ğŸ“ File Structure

```
quantization/
â”‚
â”œâ”€â”€ quant_llama32_4bit_lora.py     # Demo: load Llama 3.2 quantized + LoRA, no training
â”œâ”€â”€ qlora_finetune.py              # Full QLoRA training script (4-bit + LoRA)
â”œâ”€â”€ qlora_inference.py             # Inference using trained QLoRA adapters
â”‚
â”œâ”€â”€ models/                        # GGUF models downloaded from Hugging Face
â”œâ”€â”€ llama.cpp/                     # llama.cpp source and build directory
â”‚
â””â”€â”€ README.md                      # This documentation
```

---

# ğŸ§  Introduction to Quantization, LoRA, QLoRA & GGUF

Large Language Models (LLMs) are powerfulâ€”but **computationally expensive**.
Quantization, LoRA, and QLoRA allow us to make them:

* Smaller
* Faster
* Cheaper
* Trainable on consumer GPUs

This repository demonstrates each step in a hands-on way.

---

# ğŸ“¦ 1. What Is Quantization?

Quantization reduces model weight precision:

* FP16 â†’ INT8 / INT4
* Example: **4-bit NF4 (QLoRA)**

### âœ”ï¸ Benefits

| Benefit                 | Explanation                                |
| ----------------------- | ------------------------------------------ |
| **Lower VRAM usage**    | 4-bit weights use ~75% less VRAM than FP16 |
| **Faster inference**    | Less data â†’ faster forward pass            |
| **Cheaper deployments** | Run LLMs on small GPUs or CPUs             |
| **On-device LLMs**      | Laptops, mobile, Raspberry Pi              |

### âš ï¸ Drawbacks

| Drawback                        | Notes                                       |
| ------------------------------- | ------------------------------------------- |
| Accuracy drop                   | Low-bit quantization may degrade some tasks |
| Method matters                  | NF4, AWQ, GPTQ, GGUF all differ in fidelity |
| Some layers resist quantization | Embeddings/output heads often kept in FP16  |

---

# âš™ï¸ 2. What Is LoRA?

**LoRA (Low-Rank Adaptation)** adds trainable low-rank matrices to a frozen model.

âœ” Fine-tune huge models cheaply
âœ” Only trains adapter matrices (millions of params, not billions)
âœ” Compatible with quantized models â†’ **QLoRA**

---

# âš™ï¸ 3. What Is QLoRA?

**QLoRA = Quantized LoRA**

This allows full fine-tuning of large LLMs using **4-bit quantization** + **LoRA adapters**.

Pipeline:

1. Load base model quantized in **4-bit NF4**
2. Freeze base model
3. Insert LoRA adapters (`q_proj`, `v_proj`, etc.)
4. Train only the adapters
5. Save adapters for easy reuse (`./qlora-out/`)

This makes it possible to fine-tune:

* 7B models on a single 3090
* 33B models on consumer GPUs
* 70B models on a single A100

---

# ğŸ 4. Python Scripts in This Repository

## ğŸ”¹ `quant_llama32_4bit_lora.py` â€” **Demo (No Training)**

This script is a **teaching/demo script only**.

### What it DOES:

* Loads Llama 3.2 in **4-bit NF4** (bitsandbytes)
* Wraps the model with **LoRA adapters** (untrained)
* Demonstrates **sampling** with temperature + top-p
* Shows how quantization and LoRA *attach* to a model

### What it does NOT do:

âŒ No dataset
âŒ No training
âŒ LoRA weights remain randomly initialized

### Use case:

âœ” Demonstrate quantization + sampling
âœ” Show LoRA structure without fine-tuning
âœ” Very fast / no memory requirements

---

## ğŸ”¹ `qlora_finetune.py` â€” **Actual QLoRA Fine-Tuning**

This is the **full training script**.

### What it does:

* Loads model in **4-bit NF4**
* Adds LoRA adapters
* Loads the IMDB dataset
* Adds `labels = input_ids` for CausalLM training
* Runs HuggingFace `Trainer` to fine-tune adapters
* Saves trained adapters into `./qlora-out/`

### Use case:

âœ” Learn how QLoRA fine-tuning works
âœ” Fine-tune Llama/Qwen/Mistral on small GPU
âœ” Hands-on training with real data

---

## ğŸ”¹ `qlora_inference.py` â€” **Inference Using Trained Adapters**

This file:

* Loads the base model (4-bit or full precision)
* Loads the trained LoRA/QLoRA adapters from `./qlora-out`
* Runs generation using the improved model

### Use case:

âœ” Compare base model vs. fine-tuned model
âœ” Deploy a QLoRA model for inference
âœ” Evaluate the new behavior on custom prompts

---

# ğŸ” Summary of Differences Between Scripts

| Feature               | `quant_llama32_4bit_lora.py`    | `qlora_finetune.py`      | `qlora_inference.py`       |
| --------------------- | ------------------------------- | ------------------------ | -------------------------- |
| Quantized model       | âœ… Yes                           | âœ… Yes                    | Optional (Yes recommended) |
| LoRA adapters added   | âœ… Yes                           | âœ… Yes                    | Loads trained adapters     |
| LoRA adapters trained | âŒ No (random)                   | âœ… Yes                    | â€”                          |
| Dataset               | âŒ None                          | âœ… IMDB                   | âŒ None                     |
| Loss calculation      | âŒ No                            | âœ… Yes                    | âŒ No                       |
| Trainer used          | âŒ No                            | âœ… Yes                    | âŒ No                       |
| Saves adapters        | âŒ No                            | âœ… Saves to `./qlora-out` | âŒ No                       |
| Purpose               | Demo of quantization + sampling | Actual QLoRA fine-tuning | Use trained adapters       |

---

# ğŸ¦™ 5. GGUF / GGML + `llama.cpp`

While Transformers + QLoRA excel at training, **GGUF + llama.cpp** shine for fast inference.

### Why GGUF?

* Extremely fast CPU/GPU inference
* Supports many quantization formats
* Works without Python
* Portable, production-friendly

### Download a GGUF model

```bash
huggingface-cli login

huggingface-cli download bartowski/Llama-3.2-3B-Instruct-GGUF \
  --include "Llama-3.2-3B-Instruct-Q4_K_M.gguf" \
  --local-dir models/Llama32_3B
```

---

# âš’ï¸ 6. Build & Run `llama.cpp`

```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
cd build
```

### Run a downloaded GGUF model

```bash
bin/llama-cli \
  -m /workspace/quantization/models/Llama32_3B/Llama-3.2-3B-Instruct-Q4_K_M.gguf \
  -p "Explain quantization in simple terms."
```

### Download and run a model automatically

```bash
bin/llama-cli -hf ggml-org/gemma-3-1b-it-GGUF
```

---

# ğŸ“ 7. When to Use What?

| Task                              | Best Tool            |
| --------------------------------- | -------------------- |
| Fine-tuning on limited GPU        | **QLoRA**            |
| Teaching quantization             | QLoRA + GGUF         |
| Python-based inference            | bitsandbytes models  |
| CPU-only or lightweight inference | **GGUF + llama.cpp** |
| Deploying small/fast models       | GGUF                 |

---

# ğŸ‰ Final Notes

This repository helps students and practitioners understand:

* Quantization trade-offs
* How LoRA and QLoRA work
* Training adapters on limited hardware
* Deploying compact models with GGUF
* How to compare pipelines in practice (demo â†’ training â†’ inference â†’ deployment)
