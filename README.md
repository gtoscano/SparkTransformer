# âš¡ï¸ SparkTransformer

A lightweight transformer environment for the **NVIDIA DGX Sparkâ„¢** â€” enabling fast experimentation, inference, and fine-tuning with Hugging Face models on GPU.

This environment uses Docker + Compose with pipenv-based Python 3.12 dependency management (auto-installed on container start).

---

## ğŸš€ Overview

**SparkTransformer** provides a reproducible **GPU-accelerated training stack** built on the official:

```
nvcr.io/nvidia/pytorch:25.09-py3
```

It is designed for:

* DGX Spark GPU systems
* Transformer research & classroom use
* LoRA / QLoRA fine-tuning
* Fast prototyping for Hugging Face models
* Python 3.12 development in a clean containerized workspace

---

## âœ¨ Features

âœ… NVIDIA PyTorch 25.09 (CUDA 12.4, PyTorch 2.5, Python 3.12)
âœ… Preinstalled: `transformers`, `datasets`, `accelerate`, `peft`, `trl`, `bitsandbytes`
âœ… **pipenv auto-installation**: on every container start, if `/workspace/Pipfile` exists it installs dependencies
âœ… UID/GID passthrough to keep file permissions clean
âœ… Hugging Face token support for gated models
âœ… Supports LoRA & QLoRA training on Qwen 2.5, LLaMA, Mistral, etc.
âœ… Extensible layout for notebooks, scripts, and training modules

---

## ğŸ§± Project Structure

```
SparkTransformer/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env
â””â”€â”€ workspace/
    â”œâ”€â”€ Pipfile
    â”œâ”€â”€ train.py
    â”œâ”€â”€ train_lora.py
    â”œâ”€â”€ notebooks/
    â””â”€â”€ examples/
```

> Everything inside `workspace/` is mapped to `/workspace` in the container â€”
> this is where training and development occur.

---

# âš™ï¸ 1. Environment Setup

Create a `.env` file in the project root:

```bash
USERNAME=gtoscano
UID=$(id -u)
GID=$(id -g)
HUGGINGFACE_HUB_TOKEN=hf_xxx_your_token_here
```

âœ” Keeps HF keys out of your environment
âœ” UID/GID ensure correct file permissions

---

# ğŸ‹ 2. Build & Start the DGX Spark Environment

```bash
docker compose build
docker compose up -d
```

On startup, the container will:

* enter `/workspace`
* detect your `Pipfile`
* run:

```bash
pipenv install --deploy --system
```

You will see a message like:

```
[entrypoint] Pipfile detected in /workspace. Installing dependencies via pipenvâ€¦
```

---

# ğŸ§  3. Verify the Environment

Test CUDA + HuggingFace:

```bash
docker compose exec trainer bash -lc 'python - <<PY
import torch; from huggingface_hub import HfApi; import os
print("CUDA:", torch.cuda.is_available())
api = HfApi()
info = api.model_info("bert-base-uncased", token=os.getenv("HUGGINGFACE_HUB_TOKEN"))
print("HF OK:", info.modelId)
PY'
```

---

# ğŸ§ª 4. Training Examples (Python 3.12)

Two training scripts are included in **workspace/**:

---

## â–¶ `train.py` â€” Supervised Fine-Tuning (IMDB)

```bash
docker compose exec trainer python train.py
```

This script loads IMDB, tokenizes with DistilBERT, and fine-tunes for classification.

---

## ğŸ§© `train_lora.py` â€” LoRA Fine-Tuning (Qwen 2.5)

```bash
docker compose exec trainer python train_lora.py
```

This script:

* loads **Qwen2.5-0.5B**
* applies **LoRA adapters** via PEFT
* trains on IMDB sentiment data

Works out-of-the-box on DGX Spark GPU systems.

---

# ğŸ§  5. Run Arbitrary Transformers Workloads

### Text Generation Example

```bash
docker compose exec trainer bash -lc 'python - <<PY
from transformers import pipeline
pipe = pipeline("text-generation", model="mistralai/Mistral-7B-Instruct-v0.2")
print(pipe("The future of GPU AI is")[0]["generated_text"])
PY'
```

---

# ğŸ““ 6. Notebooks and Scripts

Place your notebooks or scripts inside:

```
workspace/notebooks/
```

Inside the container:

```bash
docker compose exec trainer python notebooks/my_notebook.py
```

---

# ğŸ§¹ 7. Shut Down

```bash
docker compose down
```

---

# ğŸ§° Utilities

| Purpose                     | Command                                                                                         |
| --------------------------- | ----------------------------------------------------------------------------------------------- |
| Test CUDA availability      | `docker compose exec trainer python -c "import torch; print(torch.cuda.is_available())"`        |
| Run default training script | `docker compose exec trainer python train.py`                                                   |
| Run LoRA training           | `docker compose exec trainer python train_lora.py`                                              |
| Update installed libraries  | `docker compose exec trainer pip install -U transformers datasets accelerate peft bitsandbytes` |
| Verify pipenv was applied   | `docker compose exec trainer pip list`                                                          |

---

# ğŸ§© Quickstart Recap

| Step | Command                            | Description                |
| ---- | ---------------------------------- | -------------------------- |
| 1    | Add HF token to `.env`             | Gives access to HF models  |
| 2    | `docker compose build`             | Build DGX Spark image      |
| 3    | `docker compose up -d`             | Start environment          |
| 4    | Add `Pipfile` to `workspace/`      | Auto-installs dependencies |
| 5    | `docker compose exec trainer bash` | Enter environment          |
| 6    | `python train.py`                  | Run a training example     |

---

# ğŸ’¡ Tips

* Store datasets and checkpoints inside `workspace/` (this persists on the host).
* Use pipenv in the host only to generate `Pipfile` + `Pipfile.lock`; installation happens in the container.
* For multi-GPU training, scale docker-compose services or use `torchrun`.

---

# ğŸ Example Quick Run

```bash
git clone https://github.com/gtoscano/SparkTransformer.git
cd SparkTransformer

# Create .env with UID/GID and HF token
cp .env.example .env

docker compose up -d

# Run LoRA training
docker compose exec trainer python train_lora.py
```

---

**Project:** gtoscano/SparkTransformer
**Platform:** NVIDIA DGX Sparkâ„¢
**Base Image:** `nvcr.io/nvidia/pytorch:25.09-py3`
**License:** MIT
**Author:** Gregorio Toscano
